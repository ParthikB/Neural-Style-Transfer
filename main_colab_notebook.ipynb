{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main - NST",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ParthikB/Neural-Style-Transfer/blob/master/main_colab_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYb4w64K_a7r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Uncomment the following code to mount the google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roKFsgZS_VmJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.15.0 # Defining which Tf version to use\n",
        "import tensorflow as tf\n",
        "import keras.backend as K\n",
        "from keras.applications.vgg19 import VGG19, preprocess_input\n",
        "from tensorflow.python.keras import models\n",
        "from keras.models import Model\n",
        "from keras.preprocessing import image\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import cv2\n",
        "import os\n",
        "import urllib\n",
        "import statistics\n",
        "from tqdm import tqdm_notebook as tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f76I4l_B_b5M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Enabling Eager Execution\n",
        "tf.enable_eager_execution()\n",
        "print(\"Eager Execution Initialized:\",tf.executing_eagerly())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lOW4Vh9ktyY",
        "colab_type": "text"
      },
      "source": [
        "# Helper Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYmdVNcLCaCD",
        "colab_type": "text"
      },
      "source": [
        "### Selecting the Feature Layers needed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUg3eIQDBgnG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining the Feature Layers we need respectively\n",
        "styleLayers = ['block1_conv2', \n",
        "               'block2_conv2', \n",
        "               'block3_conv3', \n",
        "               'block4_conv3', \n",
        "               'block5_conv3']\n",
        "\n",
        "contentLayer = ['block3_conv2']\n",
        "\n",
        "\n",
        "numContentLayers = len(contentLayer) # Number of Content Layers\n",
        "numStyleLayers   = len(styleLayers)  # Number of Style Layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyrVfwX-CeKw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining Function to import model (VGG19)\n",
        "def getModel():\n",
        "\n",
        "  # Loading Model from tf\n",
        "  model = tf.keras.applications.vgg19.VGG19(include_top=False, weights='imagenet')\n",
        "  model.trainable = False # Freezing to parameters\n",
        "\n",
        "  # Features of the Respective Layers\n",
        "  contentFeatures = [model.get_layer(name).output for name in contentLayer]\n",
        "  styleFeatures   = [model.get_layer(name).output for name in styleLayers]\n",
        "  \n",
        "  modelOutput = contentFeatures + styleFeatures\n",
        "\n",
        "  return models.Model(model.input, modelOutput)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLOL1qbSDkDY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining GRAM MATRIX\n",
        "def gram(x):\n",
        "\n",
        "  # number of channels\n",
        "  channels = int(x.shape[-1])\n",
        "  \n",
        "  # reshaping to channel first\n",
        "  a = tf.reshape(x, [-1, channels])\n",
        "  n = tf.shape(a)[0]\n",
        "  \n",
        "  # gram matrix\n",
        "  gram = tf.matmul(a, a, transpose_a=True)\n",
        "  \n",
        "  return gram / tf.cast(n, tf.float32)\n",
        "\n",
        "\n",
        "# Defining CONTENT COST\n",
        "def contentCost(contentFeatures, generateFeatures):\n",
        "  return tf.reduce_mean(tf.square(contentFeatures-generateFeatures))\n",
        "\n",
        "\n",
        "# Defining STYLE COST\n",
        "def styleCost(styleFeatures, generateFeatures):\n",
        "  styleGram = gram(styleFeatures)\n",
        "  return tf.reduce_mean(tf.square(styleGram - generateFeatures))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OHnNdN10-PE",
        "colab_type": "text"
      },
      "source": [
        "### Image Manipulations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vry1Gt1mEcnk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getFeatures(content, style, model):\n",
        "  \n",
        "  # Defining the respective outputs from our model\n",
        "  contentOutputs = model(content)\n",
        "  styleOutputs = model(style)\n",
        "  \n",
        "  # Extracting out the different features from the model output\n",
        "  contentFeatures = [contentFeature[0] for contentFeature in contentOutputs[numStyleLayers:]]\n",
        "  styleFeatures = [styleFeature[0] for styleFeature in styleOutputs[:numStyleLayers]]\n",
        "\n",
        "  return contentFeatures, styleFeatures\n",
        "\n",
        "\n",
        "def loadImage(path_to_img):\n",
        "  max_dim = 512\n",
        "  if type(path_to_img) == str:\n",
        "    img2show = Image.open(path_to_img)\n",
        "  else:\n",
        "    img2show = path_to_img\n",
        "\n",
        "  # long = max(img2show.size)\n",
        "  long = (img2show.size)\n",
        "\n",
        "  scale = max_dim/long\n",
        "  img = img2show.resize((round(img2show.size[0]*scale), round(img2show.size[1]*scale)), Image.ANTIALIAS)\n",
        "  \n",
        "  img = image.img_to_array(img)\n",
        "  \n",
        "  # We need to broadcast the image array such that it has a batch dimension \n",
        "  img = np.expand_dims(img2show, axis=0)\n",
        "  return img2show, img\n",
        " \n",
        "\n",
        "def urlToImage(url):\n",
        "  resp = urllib.request.urlopen(url)\n",
        "  img = np.asarray(bytearray(resp.read()), dtype='uint8')\n",
        "  img = cv2.imdecode(img, cv2.IMREAD_COLOR)\n",
        "  img2show = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "  img = np.expand_dims(img2show, axis=0)\n",
        "  return img2show, img\n",
        " \n",
        "\n",
        "def inputImageAndPreprocess(path):\n",
        "  # Loading the image and reshaping it according to VGG19 requirements.\n",
        "  if path[:4]=='http':\n",
        "    # print(\"Loading Image from Internet...\")\n",
        "    img2show, img = urlToImage(path)\n",
        "  else:\n",
        "    # print(\"Loading Image from Local...\")\n",
        "    img2show, img = loadImage(path)\n",
        "  \n",
        "  # Preprocessing the img according to VGG19 requirements\n",
        "  img = tf.keras.applications.vgg19.preprocess_input(img)\n",
        "\n",
        "  return img2show, img\n",
        "\n",
        "\n",
        "# Deprocessing Image to save locally\n",
        "def deprocessImage(processed_img):\n",
        "  x = processed_img.copy()\n",
        "  if len(x.shape) == 4:\n",
        "    x = np.squeeze(x, 0)\n",
        "  assert len(x.shape) == 3, (\"Input to deprocess image must be an image of \"\n",
        "                             \"dimension [1, height, width, channel] or [height, width, channel]\")\n",
        "  if len(x.shape) != 3:\n",
        "    raise ValueError(\"Invalid input to deprocessing image\")\n",
        "  \n",
        "  # perform the inverse of the preprocessiing step\n",
        "  x[:, :, 0] += 103.939\n",
        "  x[:, :, 1] += 116.779\n",
        "  x[:, :, 2] += 123.68\n",
        "  x = x[:, :, ::-1]\n",
        "\n",
        "  x = np.clip(x, 0, 255).astype('uint8')\n",
        "  return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G--g9kADOCuI",
        "colab_type": "text"
      },
      "source": [
        "### Computing the total Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3bx7mzlonff",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def totalLoss(model, lossWeights, generateImage, contentFeatures, styleFeatures):\n",
        "\n",
        "  # Extracting the respective weights\n",
        "  contentWeight, styleWeight = lossWeights\n",
        "\n",
        "  # Extracting the generate image features from the model\n",
        "  modelOutputs = model(generateImage)\n",
        "\n",
        "  # Splitting the generate Features into different categories\n",
        "  contentGenerateFeatures = modelOutputs[numStyleLayers:]\n",
        "  styleGenerateFeatures   = modelOutputs[:numStyleLayers]\n",
        "\n",
        "  # Initializing all costs with 0\n",
        "  contentCostValue, styleCostValue = 0, 0\n",
        "\n",
        "  # Defining partial weights\n",
        "  contentWeightPerLayer = 1.0 / float(numContentLayers)\n",
        "  styleWeightPerLayer = 1.0 / float(numStyleLayers)\n",
        "\n",
        "  # Computing Content Cost\n",
        "  for generateContent, combinationContent in zip(contentFeatures, contentGenerateFeatures):\n",
        "    contentCostValue += contentWeightPerLayer * contentCost(combinationContent[0], generateContent)\n",
        "\n",
        "  # Computing Style Cost for every layer\n",
        "  for generateStyle, combinationStyle in zip(styleFeatures, styleGenerateFeatures):\n",
        "    styleCostValue += styleWeightPerLayer * styleCost(combinationStyle[0], generateStyle)\n",
        "  \n",
        "\n",
        "  # Assigning the weights\n",
        "  contentCostValue *= contentWeight\n",
        "  styleCostValue *= styleWeight\n",
        "\n",
        "  # Computing the Total Loss\n",
        "  totalLossValue = contentCostValue + styleCostValue\n",
        "\n",
        "  return totalLossValue, contentCostValue, styleCostValue"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5m_rbo5OGnU",
        "colab_type": "text"
      },
      "source": [
        "### Computing the Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkuJHoGOtQkf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def computeGrads(config):\n",
        "  with tf.GradientTape() as tape:\n",
        "    allLoss = totalLoss(**config)\n",
        "\n",
        "  loss = allLoss[0]\n",
        "\n",
        "  return tape.gradient(loss, config['generateImage']), allLoss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBBVPtneLdxK",
        "colab_type": "text"
      },
      "source": [
        "### Time to get it all together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWMyORWOuPhN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining the MAIN TRAINING FUNCTION\n",
        "def runStyleTransfer(contentPath,\n",
        "                     stylePath,\n",
        "                     iterations     = 1000,\n",
        "                     SAVE_EVERY     = 0,\n",
        "                     contentWeight  = 1e3,\n",
        "                     styleWeight    = 1e-2,\n",
        "                     output_dirName = None):\n",
        "    \n",
        "  # Importing the Model\n",
        "  model = getModel()\n",
        "  for layer in model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "  if type(contentPath) == str:\n",
        "    _ , contentImage = inputImageAndPreprocess(contentPath)\n",
        "  else:\n",
        "    contentImage = contentPath\n",
        "    \n",
        "  _ ,     styleImage = inputImageAndPreprocess(stylePath)\n",
        "  \n",
        "  # Extracting out the respective features from the model\n",
        "  contentFeatures, styleFeatures = getFeatures(contentImage, styleImage, model)\n",
        "  styleFeatures = [gram(styleFeature) for styleFeature in styleFeatures]\n",
        "\n",
        "  # Creating the Generate Image\n",
        "  generateImage = contentImage\n",
        "  generateImage = tf.Variable(generateImage, dtype=tf.float32)\n",
        "\n",
        "  # Defining the Adam Optimizer\n",
        "  optimizer = tf.train.AdamOptimizer(learning_rate=5, epsilon=1e-3)\n",
        "\n",
        "  # Storing the best Image and Loss\n",
        "  bestLoss, bestImage = float('inf'), None\n",
        "\n",
        "  # Zipping the Weights\n",
        "  lossWeights = (contentWeight, styleWeight)\n",
        "  \n",
        "  # Defining the Config File\n",
        "  config = {\n",
        "    'model': model,\n",
        "    'lossWeights': lossWeights,\n",
        "    'generateImage': generateImage,\n",
        "    'contentFeatures': contentFeatures,\n",
        "    'styleFeatures': styleFeatures\n",
        "    }\n",
        "  \n",
        "  \n",
        "  normMeans = np.array([103.939, 116.779, 123.68])\n",
        "  minVals = -normMeans\n",
        "  maxVals = 255 - normMeans  \n",
        "\n",
        "  # Creating Logs to use for Plotting Later\n",
        "  global contentCostLog, styleCostLog, totalCostLog \n",
        "  contentCostLog, styleCostLog, totalCostLog = [], [], []\n",
        "\n",
        "\n",
        "  if output_dirName:\n",
        "    PATH = f'/content/drive/My Drive/Colab Notebooks/Neural Style Transfer/output/vids/{output_dirName}'\n",
        "    if not os.path.isdir(PATH):\n",
        "      os.mkdir(PATH)\n",
        "    os.chdir(PATH)\n",
        "\n",
        "  for iter in tqdm(range(iterations), leave=False):\n",
        "\n",
        "    # Computing the Grads and Loss\n",
        "    grads, allLoss = computeGrads(config)\n",
        "\n",
        "    # Extracting different kinds of Losses\n",
        "    loss, contentLoss, styleLoss = allLoss\n",
        "    \n",
        "    # Saving the respective losses in respective lists for plotting\n",
        "    contentCostLog.append(contentLoss)\n",
        "    styleCostLog.append(styleLoss)\n",
        "    totalCostLog.append(loss)\n",
        "\n",
        "    # Applying gradients to Generate Image\n",
        "    optimizer.apply_gradients([(grads, generateImage)])\n",
        "\n",
        "    # Clipping the values of Generate Image from (-255, 255)\n",
        "    clipped = tf.clip_by_value(generateImage, minVals, maxVals)\n",
        "    generateImage.assign(clipped)\n",
        "\n",
        "    # Updating the Best Image and Loss\n",
        "    if loss < bestLoss:\n",
        "      bestLoss = loss\n",
        "      bestImage = generateImage.numpy()\n",
        "\n",
        "    # Saving the Generate Image\n",
        "    if SAVE_EVERY:\n",
        "      if iter % SAVE_EVERY == 0:\n",
        "        new = cv2.cvtColor(deprocessImage(generateImage.numpy()), cv2.COLOR_RGB2BGR)\n",
        "        cv2.imwrite(f'generateImage_{iter+1}.jpg', new)\n",
        "\n",
        "    \n",
        "  if not SAVE_EVERY:\n",
        "    bestImage = deprocessImage(generateImage.numpy())\n",
        "  else:\n",
        "    # Saving the numpy Arrays to plot later\n",
        "    np.save('contentLoss.npy', contentCostLog)\n",
        "    np.save('styleLoss.npy', styleCostLog)\n",
        "    np.save('totalCostLoss.npy', totalCostLog)\n",
        "\n",
        "  return bestImage, bestLoss, output_dirName\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFMQ6znrMY3y",
        "colab_type": "text"
      },
      "source": [
        "## Visualizing our Content And Style Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpXligxdVBFZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Style Templates:\n",
        "wave = 'https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcQ8LOCDfTvcJ_V4fBdtL3R_oQn7D9P96PPzJFCksdWeKHHhyfUZ'\n",
        "seated_nude = 'https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcQ0ip7KMW5XB_qhU3cwBDDd1fjlogHfgOxw9gnVq2CqZdLwHgY3'\n",
        "shinchan = 'https://pbs.twimg.com/profile_images/452516792426975232/rOQPTVq4_400x400.png'\n",
        "\n",
        "# PATH = '/content/drive/My Drive/Colab Notebooks/Neural Style Transfer/vids/surf/'\n",
        "\n",
        "# Enter the path of the respective Images\n",
        "contentImagePath = wave\n",
        "styleImagePath   = seated_nude\n",
        "\n",
        "content, _ = inputImageAndPreprocess(contentImagePath)\n",
        "style, _   = inputImageAndPreprocess(styleImagePath)\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(content)\n",
        "plt.title('Content Image')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(style)\n",
        "plt.title('Style Image')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "117gi_T-MneQ",
        "colab_type": "text"
      },
      "source": [
        "# **Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPGdWaG43zMD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ITERATIONS = 1\n",
        "DIR_NAME   = 'seated_nude'\n",
        "\n",
        "\n",
        "bestImage, bestLoss, output_dirName = runStyleTransfer(contentImagePath,\n",
        "                                                      styleImagePath,\n",
        "                                                      iterations=ITERATIONS,\n",
        "                                                      SAVE_EVERY = 0,\n",
        "                                                      contentWeight = 1,\n",
        "                                                      styleWeight= 0.8,\n",
        "                                                      output_dirName = None)\n",
        "\n",
        "# Output Cleared for Fairness"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzkI-DB6M5Wj",
        "colab_type": "text"
      },
      "source": [
        "#### Visualizing the Best Image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ywRuBn7fHpkP",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow(bestImage)\n",
        "plt.title('Generated Image')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BIqrewkNBxJ",
        "colab_type": "text"
      },
      "source": [
        "#### Plotting the Cost Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpxrPjPH-AJ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(23, 8))\n",
        "plt.plot(totalCostLog,   linewidth=3, label='total loss')\n",
        "plt.plot(styleCostLog,   linewidth=1, label='style loss')\n",
        "plt.plot(contentCostLog, linewidth=2, label='content loss')\n",
        "# plt.plot(learning_curve_tv, linewidth=2, label='total variation loss')\n",
        "plt.title(\"Learning curve\")\n",
        "plt.ylabel(\"error\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.yscale(\"log\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f12hKz_q9LJa",
        "colab_type": "text"
      },
      "source": [
        "#Video Conversion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsD-NPZavu9v",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Parameters:\n",
        "Video_path = \"/content/drive/My Drive/Colab Notebooks/Neural Style Transfer/vids/surf.mp4\" #@param {type:\"string\"}\n",
        "\n",
        "style_path = \"https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcQ8LOCDfTvcJ_V4fBdtL3R_oQn7D9P96PPzJFCksdWeKHHhyfUZ\" #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "fps_quality = \"Just show me the thing already\" #@param [\"Super Duper\", \"Fancier than Average\", \"Ya ok Whatever\", \"Just show me the thing already\"]\n",
        "\n",
        "style_quality = 100 #@param {type:\"slider\", min:100, max:1000, step:100}\n",
        "\n",
        "if fps_quality == 'Super Duper':\n",
        "  skip_frame_every = 1\n",
        "elif fps_quality == 'Fancier than Average':\n",
        "  skip_frame_every = 3\n",
        "elif fps_quality == 'Ya ok Whatever':\n",
        "  skip_frame_every = 6\n",
        "else:\n",
        "  skip_frame_every = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PMZLRMvT5V8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_frames_out_of_the_video(vid_path):\n",
        "  cam = cv2.VideoCapture(vid_path)\n",
        "\n",
        "  currentframe = 1\n",
        "  frames = []\n",
        "\n",
        "  while(True):   \n",
        "      # reading from frame \n",
        "      ret, frame = cam.read() \n",
        "      \n",
        "      if ret:\n",
        "          frame = np.expand_dims(frame, axis=0)\n",
        "          frames.append(frame.astype('float32'))\n",
        "          print(currentframe, end='\\r') \n",
        "          currentframe += 1\n",
        "      else: \n",
        "          break\n",
        "  print('Frames generated..!')\n",
        "  return frames"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KbvxNOzx3sf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VID_PATH   = '/content/drive/My Drive/Colab Notebooks/Neural Style Transfer/vids/surf.mp4'\n",
        "OUTPUT_DIR = '/content/drive/My Drive/Colab Notebooks/Neural Style Transfer/output/vids/'\n",
        "OUTPUT_FILE_NAME = 'trying_aivayy'\n",
        "\n",
        "generated_frames = []\n",
        "vid_frames = extract_frames_out_of_the_video(VID_PATH)\n",
        "\n",
        "for frame_number in tqdm(range(0, len(vid_frames), skip_frame_every)):\n",
        "\n",
        "  content = vid_frames[frame_number]\n",
        "\n",
        "  bestImage, bestLoss, output_dirName = runStyleTransfer(content,\n",
        "                                                        style_path,\n",
        "                                                        iterations=style_quality,\n",
        "                                                        contentWeight = 1,\n",
        "                                                        styleWeight= 0.8)\n",
        "  generated_frames.append(bestImage)\n",
        "  height, width, channels = bestImage.shape\n",
        "\n",
        "\n",
        "print('Developing the Video')\n",
        "out = cv2.VideoWriter(OUTPUT_DIR + OUTPUT_FILE_NAME + '.avi', cv2.VideoWriter_fourcc(*'DIVX'), 15, (width,height))\n",
        "for i in (range(len(generated_frames))):\n",
        "    out.write(generated_frames[i])\n",
        "out.release()\n",
        "\n",
        "print('Video Converted and Saved Succesfully!')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}